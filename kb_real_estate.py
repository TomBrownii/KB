# -*- coding: utf-8 -*-
"""KB Real Estate

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qnfdO_EgElV99WM94jfQp5n5L5rqpDSf
"""

!pip install pdfplumber
!pip install transformers
!pip install --upgrade transformers
!pip install PyPDF2
!pip install sentencepiece
!pip install rouge

"""**라이브러리 **"""

import PyPDF2
from pdfminer.high_level import extract_text
import os
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration
import torch
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import matplotlib.pyplot as plt
import re
import rouge
from rouge import Rouge
import pdfplumber

from google.colab import drive
drive.mount('/content/drive')

"""**Data 전처리 **"""

#Extract Second and Other Page
def extract_pdf_text(pdf_path):
    text = ""
    with open(pdf_path, "rb") as pdf_file:
        reader = PyPDF2.PdfReader(pdf_file)
        for page_number in range(1, len(reader.pages)):
            text += reader.pages[page_number].extract_text()
            text = re.sub(r'[a-zA-Z]', '', text)
            text = text.replace('', '')
            text = text.replace('▲', '')
            text = text.replace('①','')
            text = text.replace('•','')
            text = text.replace('【】','')
            text = text.replace('②','')
            text = text.replace('', '')
            text = text.replace('◼','')
            text = text.replace('▲','')
            text = text.replace('Executive Summary', '')
            text = text.replace('Summary', '')
            text = text.replace('','')
            text = text.replace('◦','')
            text = text.replace('∙','')
            text = text.replace('< 그 림 > ','')
            text = text.replace('( 관 련   기 사 ) ','')
            text = text.replace('[ 관 련   기 사 ] ','')
            text = text.replace('관 련   기 사','')
            text = text.replace(':','')
            text = text.replace('[ 표 ]','')
            text = text.replace('[','')
            text = text.replace(']','')
            text = text.replace('\n','')
            text = text.replace('< 요   약 >','')
            text = text.replace('· ','')
    return text

#Extract First Page
def extract_first_page_text(pdf_path):
    text = ""
    with open(pdf_path, "rb") as pdf_file:
        reader = PyPDF2.PdfReader(pdf_file)
        if len(reader.pages) > 0:
            text = reader.pages[0].extract_text()
            text = re.sub(r'[a-zA-Z]', '', text)
            text = text.replace('', '')
            text = text.replace('▲', '')
            text = text.replace('①','')
            text = text.replace('•','')
            text = text.replace('【】','')
            text = text.replace('②','')
            text = text.replace('', '')
            text = text.replace('◼','')
            text = text.replace('▲','')
            text = text.replace('Executive Summary', '')
            text = text.replace('Summary', '')
            text = text.replace('','')
            text = text.replace('◦','')
            text = text.replace('∙','')
            text = text.replace('< 그 림 > ','')
            text = text.replace('( 관 련   기 사 ) ','')
            text = text.replace('[ 관 련   기 사 ] ','')
            text = text.replace('관 련   기 사','')
            text = text.replace(':','')
            text = text.replace('[ 표 ]','')
            text = text.replace('[','')
            text = text.replace(']','')
            text = text.replace('\n','')
            text = text.replace('< 요   약 >','')
            text = text.replace('· ','')
    return text

"""**Data 불러오기 **"""

#Make Data -> Train Data Ram 한계
pdf1_directory = "/content/drive/MyDrive/KB REAL ESTATE/"
pdf2_directory = "/content/drive/MyDrive/KB Test/"
text1_data = []
label1_data = []
text2_data = []
label2_data = []

#Labeling
for pdf1_filename in os.listdir(pdf1_directory):
    if pdf1_filename.endswith(".pdf"):
        pdf1_path = os.path.join(pdf1_directory, pdf1_filename)
        pdf1_text = extract_pdf_text(pdf1_path)
        label1_text = extract_first_page_text(pdf1_path)
        text1_data.append(pdf1_text)
        label1_data.append(label1_text)

for pdf2_filename in os.listdir(pdf2_directory):
    if pdf2_filename.endswith(".pdf"):
        pdf2_path = os.path.join(pdf2_directory, pdf2_filename)
        pdf2_text = extract_pdf_text(pdf2_path)
        label2_text = extract_first_page_text(pdf2_path)
        text2_data.append(pdf2_text)
        label2_data.append(label2_text)

#Data Processing Check
print(text2_data,'\n')
print(label2_data)

"""**Main Model**"""

# 데이터를 학습과 테스트 데이터로 분할
train_data = text1_data
train_labels = label1_data
test_data = text2_data
test_labels = label2_data

# Pretrained Kobart 모델 및 토크나이저 불러오기
tokenizer = PreTrainedTokenizerFast.from_pretrained('digit82/kobart-summarization')
model = BartForConditionalGeneration.from_pretrained('digit82/kobart-summarization')

# 학습 데이터셋 클래스 정의
class CustomDataset(Dataset):
    def __init__(self, data, labels, tokenizer, max_source_length=512, max_target_length=256):
        self.data = data
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_source_length = max_source_length
        self.max_target_length = max_target_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        input_text = self.data[idx]
        target_text = self.labels[idx]


        inputs = self.tokenizer.encode("summarize: " + input_text, return_tensors="pt", max_length=self.max_source_length, truncation=True)
        labels = self.tokenizer.encode(target_text, return_tensors="pt", max_length=self.max_target_length, truncation=True)

        input_ids = inputs[0]
        label_ids = labels[0]

        attention_mask = torch.ones_like(input_ids)

        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": label_ids,
        }

# 학습 데이터셋 생성
train_dataset = CustomDataset(train_data, train_labels, tokenizer)
test_dataset = CustomDataset(test_data, test_labels, tokenizer)

# 데이터로더 설정
train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=True)

# 모델 학습 설정
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)

num_epochs = 150
losses = []

# Assuming you have defined your model, optimizer, and train_dataloader

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    num_batches = 0
    for batch in tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        batch = {k: v.to(device) for k, v in batch.items()}
        optimizer.zero_grad()

        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        labels = batch["labels"]

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss = loss.mean()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        num_batches += 1
    avg_loss = total_loss / num_batches
    losses.append(avg_loss)
    print(f"Epoch {epoch+1}/{num_epochs} - Average Loss: {avg_loss:.4f}")

"""**Epoch-Loss 그래프**"""

#Graph
plt.title('Epochs-Loss')
plt.plot(range(1, num_epochs+1), losses)
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.show()

"""**요약문 출력 **"""

model.eval()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

summaries = []
for test_text, test_label in zip(test_data, test_labels):
    raw_input_ids = tokenizer.encode(test_text)
    if len(raw_input_ids) > 256:
        raw_input_ids = raw_input_ids[:256]
    input_ids = [tokenizer.bos_token_id] + raw_input_ids
    input_ids = torch.tensor([input_ids]).to(device)  # Move input tensor to the appropriate device
    summary_ids = model.generate(input_ids, max_length=256, repetition_penalty=2.0, eos_token_id=2)
    summary_tokens = tokenizer.decode(summary_ids.squeeze().tolist(), skip_special_tokens=True)
    summaries.append(summary_tokens)

for i in range(len(summaries)):
    print(summaries[i])

"""**Rouge 성능 평가 **"""

#rouge Evaluation
rouge = Rouge()
rouge_scores = rouge.get_scores(summaries, test_labels)
rouge_scores_single = rouge_scores[0]

print(f'ROUGE-1 : {rouge_scores_single["rouge-1"]["f"]:.4f}')
print(f'ROUGE-2 : {rouge_scores_single["rouge-2"]["f"]:.4f}')
print(f'ROUGE-L : {rouge_scores_single["rouge-l"]["f"]:.4f}')

"""**최종 결과 그래프**"""

epochs = [1, 10, 30, 50 ,100, 150, 200]
rouge_1_scores = [0.1778, 0.1905, 0.2288, 0.2426, 0.2761, 0.3213, 0.3020]
rouge_2_scores = [0.0107, 0.0323, 0.0829, 0.0441, 0.0506,0.1308, 0.1056]
rouge_l_scores = [0.1714, 0.1714, 0.2288, 0.2295, 0.2559, 0.3148, 0.2819]

plt.plot(epochs, rouge_1_scores, marker='o', color='red', label='ROUGE-1')
plt.plot(epochs, rouge_2_scores, marker='o', color='blue', label='ROUGE-2')
plt.plot(epochs, rouge_l_scores, marker='o', color='green', label='ROUGE-L')

plt.title('ROUGE Scores over Epochs')
plt.xlabel('Epoch')
plt.ylabel('ROUGE Score')
plt.xticks(epochs)
plt.grid()
plt.legend()

plt.show()